from openai import OpenAI
import os
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ OPENAI_API_KEY ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def generate_gpt_answer(question, context):
    prompt = f"""
        ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì•„ë˜ ë¬¸ë§¥(Context)ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.
        ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ìƒìƒí•˜ì§€ ë§ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
        ğŸ“Œ ë¬¸ë§¥ì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ë‹¨ê³„ë³„ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        íŠ¹íˆ "í•´ê²° ë°©ë²•", "ì˜ˆë°© íŒ"ì„ ë¹ ì§ì—†ì´ ë°˜ì˜í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.

        [ë¬¸ë§¥ ì •ë³´]
        {context}

        [ì§ˆë¬¸]
        {question}
        """.strip()

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            { "role": "system", "content": "ì¹œì ˆí•œ í•œêµ­ì–´ í™ˆì¼€ì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=1024
    )
    return response.choices[0].message.content.strip()